{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/educbd/ML_/blob/main/Lab_3_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 3 Part 2 - Task 1: Parameters in CNN (5 Marks)"
      ],
      "metadata": {
        "id": "VIT-maBq2bbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the model we have created in **Lab 3 Part 1 Exercise**: Early Stopping with Callbacks, calculate the number of parameters by hand for each layer and compare to the output of model.summary() and print the model summary.\n",
        "- Then print the model summary of **Exercise 7 in Lab 1**\n",
        "- Now compare the Model you created in **Exercise 7 in Lab 1**,\n",
        "  - Compare the Parameters of the models\n",
        "\n",
        "  - Compare Model Performance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tWo32rVm8TGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "EHVDjeKNbuEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "e2QDmWq5POZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the first 10,000 samples of our training data as our validation set\n",
        "val_data = train_data[:10000]\n",
        "val_labels = train_labels[:10000]\n",
        "\n",
        "# Use the remainder of the original training data for actual training\n",
        "partial_train_data = train_data[10000:]\n",
        "partial_train_labels = train_labels[10000:]"
      ],
      "metadata": {
        "id": "OD2jaJiaPRdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the pixel values so they lie in the range of 0-1\n",
        "partial_train_data = partial_train_data / 255.\n",
        "val_data = val_data / 255.\n",
        "test_data = test_data /255."
      ],
      "metadata": {
        "id": "BJ2GvaJePVWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_train_data = np.expand_dims(partial_train_data, axis=3)\n",
        "val_data = np.expand_dims(val_data, axis=3)\n",
        "test_data = np.expand_dims(test_data, axis=3)"
      ],
      "metadata": {
        "id": "k96MNgg1PZjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_train_labels = to_categorical(partial_train_labels)\n",
        "val_labels = to_categorical(val_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "metadata": {
        "id": "hDNirhpePbwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the early stopping callback\n",
        "# This helps stop training if validation loss doesn't improve for 5 consecutive epochs\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Create the neural network model\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(filters=16,\n",
        "                                  kernel_size=(3, 3),\n",
        "                                  strides=1, padding='same',\n",
        "                                  activation='relu',\n",
        "                                  input_shape=(28, 28, 1)))\n",
        "model.add(tf.keras.layers.Conv2D(filters=32,\n",
        "                                  kernel_size=(3, 3),\n",
        "                                  strides=2,\n",
        "                                  padding='valid',\n",
        "                                  activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=64,\n",
        "                                  kernel_size=(3, 3),\n",
        "                                  strides=1,\n",
        "                                  padding='same',\n",
        "                                  activation='relu'))\n",
        "model.add(tf.keras.layers.Conv2D(filters=128,\n",
        "                                  kernel_size=(3, 3),\n",
        "                                  strides=1,\n",
        "                                  padding='valid',\n",
        "                                  activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fit the model\n",
        "history = model.fit(\n",
        "    partial_train_data,\n",
        "    partial_train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=256,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    callbacks=[callback],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
        "\n",
        "# Print test loss and accuracy\n",
        "print(f'Test Loss: {test_loss}')\n",
        "print(f'Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "PWci6yrl2Zlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23475022-b8c3-494e-8759-6064b4ebe20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "196/196 [==============================] - 106s 539ms/step - loss: 0.2850 - accuracy: 0.9157 - val_loss: 0.0950 - val_accuracy: 0.9730\n",
            "Epoch 2/20\n",
            "196/196 [==============================] - 112s 572ms/step - loss: 0.0536 - accuracy: 0.9829 - val_loss: 0.0558 - val_accuracy: 0.9834\n",
            "Epoch 3/20\n",
            "196/196 [==============================] - 108s 552ms/step - loss: 0.0312 - accuracy: 0.9908 - val_loss: 0.0720 - val_accuracy: 0.9801\n",
            "Epoch 4/20\n",
            "196/196 [==============================] - 108s 552ms/step - loss: 0.0213 - accuracy: 0.9930 - val_loss: 0.0655 - val_accuracy: 0.9834\n",
            "Epoch 5/20\n",
            "196/196 [==============================] - 108s 549ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.0589 - val_accuracy: 0.9866\n",
            "Epoch 6/20\n",
            "196/196 [==============================] - 113s 579ms/step - loss: 0.0108 - accuracy: 0.9964 - val_loss: 0.0605 - val_accuracy: 0.9875\n",
            "Epoch 7/20\n",
            "196/196 [==============================] - 108s 554ms/step - loss: 0.0081 - accuracy: 0.9973 - val_loss: 0.0500 - val_accuracy: 0.9882\n",
            "Epoch 8/20\n",
            "196/196 [==============================] - 109s 555ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0595 - val_accuracy: 0.9871\n",
            "Epoch 9/20\n",
            "196/196 [==============================] - 105s 535ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0680 - val_accuracy: 0.9893\n",
            "Epoch 10/20\n",
            "196/196 [==============================] - 106s 539ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0690 - val_accuracy: 0.9889\n",
            "Epoch 11/20\n",
            "196/196 [==============================] - 110s 558ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0763 - val_accuracy: 0.9894\n",
            "Epoch 12/20\n",
            "196/196 [==============================] - 106s 540ms/step - loss: 0.0031 - accuracy: 0.9990 - val_loss: 0.0962 - val_accuracy: 0.9835\n",
            "313/313 [==============================] - 7s 24ms/step - loss: 0.0712 - accuracy: 0.9855\n",
            "Test Loss: 0.07116775959730148\n",
            "Test Accuracy: 0.9854999780654907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zshMw7d7t87A",
        "outputId": "46d1c808-6808-4358-c32c-4c971688bee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 13, 13, 32)        4640      \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 11, 11, 128)       73856     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 15488)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               1982592   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2081034 (7.94 MB)\n",
            "Trainable params: 2081034 (7.94 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the parameters by hand\n",
        "layer1 = np.array([28,28,16]) #padding is same so the output height and width is the same of input, just the filter number is added\n",
        "layer2 = np.array([np.floor((((28+(2*0)-3)/2)+1)),np.floor((((28+(2*0)-3)/2)+1)),32])\n",
        "layer3 = layer2 + np.array([0,0,(64-32)])#padding is same so the output height and width is the same of previous layer, just the filter number changes\n",
        "layer4 = np.array([np.floor((((13+(2*0)-3)/1)+1)),np.floor((((13+(2*0)-3)/1)+1)),128])\n",
        "\n",
        "# Exhibting the parameters calculated by hand - Lab 3 Part 1\n",
        "print(f'Layer 1: {layer1}')\n",
        "print(f'Layer 2: {layer2}')\n",
        "print(f'Layer 3: {layer3}')\n",
        "print(f'Layer 4: {layer4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oko9matMa3Zj",
        "outputId": "3125a3ab-1fc8-44a4-e2e0-7aeb8eb34dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1: [28 28 16]\n",
            "Layer 2: [13. 13. 32.]\n",
            "Layer 3: [13. 13. 64.]\n",
            "Layer 4: [ 11.  11. 128.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images2, train_labels2), (test_images2, test_labels2) = mnist.load_data()"
      ],
      "metadata": {
        "id": "aI7LPi_M01MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary for Lab 1 - Exercise 7\n",
        "(train_images2, train_labels2), (test_images2, test_labels2) = mnist.load_data()\n",
        "\n",
        "train_images2 = train_images2.astype('float32') / 255\n",
        "train_images2 = train_images2.reshape((60000, 28 * 28))\n",
        "\n",
        "test_images2 = test_images2.astype('float32') / 255\n",
        "test_images2 = test_images2.reshape((10000, 28 * 28))\n",
        "\n",
        "train_labels2 = to_categorical(train_labels)\n",
        "test_labels2 = to_categorical(test_labels)\n",
        "\n",
        "network = Sequential()\n",
        "network.add(Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(Dense(64, activation='sigmoid'))\n",
        "network.add(Dense(10, activation='softmax'))\n",
        "\n",
        "network.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "kiWuIiRYOioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network.fit(train_images2, train_labels2, epochs=10, batch_size=32, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGvRKa0d1uZB",
        "outputId": "9b396ef2-ff7e-49f5-84ce-633a6c0db38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2464 - accuracy: 0.9344\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0836 - accuracy: 0.9749\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0539 - accuracy: 0.9833\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0369 - accuracy: 0.9884\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0277 - accuracy: 0.9912\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0196 - accuracy: 0.9939\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0162 - accuracy: 0.9952\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0141 - accuracy: 0.9955\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0112 - accuracy: 0.9963\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0093 - accuracy: 0.9972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a2478369f60>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "network.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDuhltlbuYUX",
        "outputId": "a7e41653-a3c1-4f11-c15c-4807cf85d04c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 435402 (1.66 MB)\n",
            "Trainable params: 435402 (1.66 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing the models:**\n",
        "\n",
        "The model we created in Lab 3 part 1 using convolution layers has more then 2 million parameters, this is almost 5 times the number of parameters we had for the model we created in Lab 1 exercise 7 using only dense layers.\n",
        "\n",
        "The validation accuracy of the model in Lab 1 is higher, achieving 99,72% against 98.35% from the other model."
      ],
      "metadata": {
        "id": "WklmsCa7Cr7L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maXaHAcw2TsB"
      },
      "source": [
        "# Lab 3 Part 2 - Task 2: CIFAR-10 Challenge (10 Marks)\n",
        "\n",
        "In this lab you will experiment with whatever ConvNet architecture/design you'd like on [CIFAR-10 image dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "## Exercise  1: Creating the network\n",
        "\n",
        "**Goal:** After training, your model should achieve **at least 80%** accuracy on a **validation** set within 20 epochs. (Or as close as possible as long as there is demonstrated effort to achieve this goal.)\n",
        "\n",
        "**Data split** The training set should consist of 40000 images, the validation set should consist of 10000 images, and the test set should consist of the remaining 10000 images. **Please use the Keras `load_data()` function to import the data set.**\n",
        "\n",
        "\n",
        "### Some things you can try:\n",
        "- Different number/type of layers\n",
        "- Different filter sizes\n",
        "- Adjust the number of filters used in any given layer\n",
        "- Try various pooling strategies\n",
        "- Consider using batch normalization\n",
        "- Check if adding regularization helps\n",
        "- Consider alternative optimizers\n",
        "- Try different activation functions\n",
        "\n",
        "\n",
        "### Tips for training\n",
        "When building/tuning your model, keep in mind the following points:\n",
        "\n",
        "- This is experimental, so be driven by results achieved on the validation set as opposed to what you have heard/read works well or doesn't\n",
        "- If the hyperparameters are working well, you should see improvement in the loss/accuracy within approximately one epoch\n",
        "- For hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all\n",
        "- Once you have found some sets of hyperparameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
        "- Prefer random search to grid search for hyperparameters\n",
        "- You should use the validation set for hyperparameter search and for evaluating different architectures\n",
        "- The test set should only be used at the very end to evaluate your final model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIOVFQ0m2TsG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the first 10,000 samples of our training data as our validation set\n",
        "val_data = train_data[:10000]\n",
        "val_labels = train_labels[:10000]\n",
        "\n",
        "# Use the remainder of the original training data for actual training\n",
        "partial_train_data = train_data[10000:]\n",
        "partial_train_labels = train_labels[10000:]"
      ],
      "metadata": {
        "id": "Zgq27fRWbVpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling pixel values so they lie in the range of 0-1\n",
        "partial_train_data = partial_train_data / 255.\n",
        "val_data = val_data / 255.\n",
        "test_data = test_data /255.\n",
        "\n",
        "print(partial_train_data.shape)\n",
        "print(val_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdG2zpg6bamE",
        "outputId": "617d26c9-526d-4cca-9257-516c87e66b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(partial_train_labels.shape)\n",
        "print(val_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlm5GGctbcul",
        "outputId": "389b4a27-1309-4135-a965-f86805b78d88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 1)\n",
            "(10000, 1)\n",
            "(10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential([\n",
        "    Conv2D(filters=32,\n",
        "           kernel_size=(3, 3),\n",
        "           padding='same',\n",
        "           strides = (1, 1),\n",
        "           activation='relu',\n",
        "           input_shape=(32, 32, 3)),\n",
        "    BatchNormalization(axis=-1, momentum=0.99, epsilon=0.000001),\n",
        "    Conv2D(filters=32,\n",
        "           kernel_size=(3, 3),\n",
        "           strides=1,\n",
        "           padding='same',\n",
        "           activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2),\n",
        "                 strides=2),\n",
        "\n",
        "    Conv2D(filters=64,\n",
        "           kernel_size=(3, 3),\n",
        "           strides=1,\n",
        "           padding='same',\n",
        "           activation='relu'),\n",
        "    Conv2D(filters=64,\n",
        "           kernel_size=(3, 3),\n",
        "           strides=1,\n",
        "           padding='same',\n",
        "           activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2),\n",
        "                 strides=2),\n",
        "\n",
        "    Conv2D(filters=128,\n",
        "           kernel_size=(3, 3),\n",
        "           strides=1,\n",
        "           padding='same',\n",
        "           activation='relu'),\n",
        "    Conv2D(filters=128,\n",
        "           kernel_size=(3, 3),\n",
        "           strides=1,\n",
        "           padding='same',\n",
        "           activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2),\n",
        "                 strides=2),\n",
        "\n",
        "\n",
        "    Flatten(name = \"Classifier_Flatten\"),\n",
        "    Dense(128, activation='relu', name = \"classifier_FC_1\"),\n",
        "    Dense(10, activation='softmax', name = \"classifier_output\")\n",
        "])"
      ],
      "metadata": {
        "id": "6Sh34eSIbjaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "               optimizer=keras.optimizers.RMSprop(),\n",
        "               metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "fBKj41bebnOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callback = EarlyStopping(monitor='val_loss',\n",
        "                         patience=5,\n",
        "                         restore_best_weights=True)\n",
        "history = model2.fit(partial_train_data,\n",
        "                    partial_train_labels,\n",
        "                    epochs=20,\n",
        "                    batch_size=256,\n",
        "                    validation_data=(val_data, val_labels),\n",
        "                    callbacks=[callback],\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IX_KJnHdAnK",
        "outputId": "90e9d541-79f6-45d8-8df7-2b597b8856c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - 246s 2s/step - loss: 1.7808 - accuracy: 0.3431 - val_loss: 2.1126 - val_accuracy: 0.3766\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - 239s 2s/step - loss: 1.2926 - accuracy: 0.5361 - val_loss: 1.7713 - val_accuracy: 0.4897\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - 237s 2s/step - loss: 1.0321 - accuracy: 0.6356 - val_loss: 1.3610 - val_accuracy: 0.5643\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - 234s 1s/step - loss: 0.8505 - accuracy: 0.7017 - val_loss: 1.0183 - val_accuracy: 0.6430\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - 237s 2s/step - loss: 0.7109 - accuracy: 0.7510 - val_loss: 0.8670 - val_accuracy: 0.7002\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - 233s 1s/step - loss: 0.5846 - accuracy: 0.7966 - val_loss: 1.3775 - val_accuracy: 0.5982\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - 232s 1s/step - loss: 0.4858 - accuracy: 0.8299 - val_loss: 0.8657 - val_accuracy: 0.7030\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - 231s 1s/step - loss: 0.3885 - accuracy: 0.8659 - val_loss: 0.9909 - val_accuracy: 0.7022\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - 231s 1s/step - loss: 0.3047 - accuracy: 0.8939 - val_loss: 1.0238 - val_accuracy: 0.7235\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - 238s 2s/step - loss: 0.2360 - accuracy: 0.9165 - val_loss: 0.9865 - val_accuracy: 0.7280\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - 232s 1s/step - loss: 0.1830 - accuracy: 0.9360 - val_loss: 1.0419 - val_accuracy: 0.7455\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - 231s 1s/step - loss: 0.1623 - accuracy: 0.9451 - val_loss: 1.3123 - val_accuracy: 0.7063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64BbG8Y62TsH"
      },
      "source": [
        "## Exercise 2: Describe What you did\n",
        "\n",
        "All the work you did leading up to your final model should be summarized in this section. This should be a logical and well-organized summary of the various experiments that were tried in **Lab 3 Part 2 - Task 2:Exercise 1**, and should be captured in **table format**. Upon reading this section I should understand what you tried, the reasoning behind trying it, any quantitative values that correspond to what you tried, and the results.\n",
        "\n",
        "See [this guide](https://www.datacamp.com/community/tutorials/markdown-in-jupyter-notebook) for how to format markdown cells in Jupyter notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa8p0Nkj2TsI"
      },
      "source": [
        "| Data code | Why do we choose the code? | What is the funtion? |\n",
        "| :- | :- | :- |  \n",
        "|   To Normalization   |   Normalizing the pixel values to be in the range of 0 to 1 (usually by dividing by 255) helps in training the neural network. | To ensure that the model doesn't assign too much importance to any one feature (pixel value) and can learn more effectively   |\n",
        "| one-hot encode  |To converts the integer class labels into binary vectors where each vector has a 1 in the position corresponding to the class and zeros in all other positions    | neural network understand that the classes are mutually exclusive, and to  improving the performance of the model during training.  |\n",
        "|  Splitting the data  | Splitting the X_train and the corresponding x_test and y_test for testing    | This data splitting allows you to train and validate the model's performance during training effectively. |\n",
        "| creating the Model  | The model consists of a series of layers: two convolutional layers with 32 and 64 filters, respectively, using a (3, 3) kernel and ReLU activation, followed by max-pooling layers with a (2, 2) pool size; then, another convolutional layer with 128 filters and ReLU activation, followed by max-pooling. After flattening the feature maps, there's a dense layer with 256 neurons and ReLU activation, along with batch normalization, and finally, a dense output layer with 10 neurons using softmax activation.  | The model is to process and classify images using a convolutional neural network with multiple layers, ultimately assigning one of ten possible categories to each input image.|\n",
        "| Compiling the Model | model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) | To  configures the neural network for training by specifying the optimizer ('adam'), the loss function ('categorical_crossentropy'), and the evaluation metric ('accuracy').   |\n",
        "| Fitting the model | It is used to train the neural network   |  takes the training data x_train and corresponding labels y_train. The validation_data parameter is used for validation during training. The epochs parameter defines the number of training iterations, and verbose controls the level of output during training (1 for progress updates). The training progress and results are stored in the history variable for later analysis. |\n",
        "|  Evaluating the Model  |  To evaluate the trained neural network on a test dataset, in this case, x_test and y_test  | WHence, It calculates the loss and accuracy of the model's predictions on the test data. The results, including test loss and accuracy, are stored in the variables test_loss and test_accuracy. The final line prints the test accuracy in percentage format with two decimal places.  |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}