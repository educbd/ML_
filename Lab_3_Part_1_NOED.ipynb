{"cells":[{"cell_type":"markdown","source":["# Lab 3 Part 1: Convolutions in Keras\n","The code below will allow you to play with a single convolutional layer in Keras. Take a look at the documentation for the Conv2D layer, which is also where the original code came from."],"metadata":{"id":"TR2CPIZ9tNDE"},"id":"TR2CPIZ9tNDE"},{"cell_type":"markdown","source":["---\n","\n","## **Group Number: 08**\n","\n","Name  | Student ID\n","-------------------|------------------\n","Noushin Asadsamani      | 0829532\n","Eduardo Chavez Barrientos      | 0828349\n","\n","\n","---"],"metadata":{"id":"Y4JqCp9fXnSG"},"id":"Y4JqCp9fXnSG"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ppEhKpqDbbC","executionInfo":{"status":"ok","timestamp":1697947365595,"user_tz":240,"elapsed":705,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"fb3f978e-0ec3-4537-eda5-282e0c438b6a"},"id":"_ppEhKpqDbbC","execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Conv2D"],"metadata":{"id":"wO83RrBkuB0F","executionInfo":{"status":"ok","timestamp":1697947366956,"user_tz":240,"elapsed":5,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"wO83RrBkuB0F","execution_count":83,"outputs":[]},{"cell_type":"code","source":["input_shape = (4, 28, 28, 3)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=2,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 28, 28, 3))(x)\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyXPGKnPuKWc","outputId":"d0a760f8-1fd0-43f6-98b9-7544018d4d56","executionInfo":{"status":"ok","timestamp":1697947368483,"user_tz":240,"elapsed":119,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"VyXPGKnPuKWc","execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 26, 26, 2])"]},"metadata":{},"execution_count":84}]},{"cell_type":"markdown","source":["Here is a brief explanation of the above code:\n","![An image](https://raw.githubusercontent.com/educbd/ML_/main/Convolution_notebook-2.jpg)\n","\n"],"metadata":{"id":"K1ciM03Ct9A2"},"id":"K1ciM03Ct9A2"},{"cell_type":"markdown","source":["# Exercises\n","In the code above, make changes to:\n","\n","```\n","1. input  ‚Ñé\n","2. input  ùë§\n","3. input  ùëõùëê\n","4. number of filters\n","5. kernel size (same as filter size)\n","\n","```\n","\n","\n","For each change, calculate the dimensions of the output (y.shape) by hand, including drawing a diagram (as shown below).\n","![An image](https://raw.githubusercontent.com/educbd/ML_/main/Convolution_notebook-1.jpg)\n"],"metadata":{"id":"JMlk6aIouaIF"},"id":"JMlk6aIouaIF"},{"cell_type":"markdown","source":["## **1) Changes to input  ‚Ñé**\n","```\n"," - ‚Ñé = 12\n","```"],"metadata":{"id":"lBbIuZ1zMKBw"},"id":"lBbIuZ1zMKBw"},{"cell_type":"code","source":["input_shape = (4, 12, 28, 3)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=2,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 12, 28, 3))(x)\n","\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2nOkCvyTKSTz","executionInfo":{"status":"ok","timestamp":1697947376420,"user_tz":240,"elapsed":109,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"3d02c3d8-151e-484f-de5f-90d0f78b1875"},"id":"2nOkCvyTKSTz","execution_count":85,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 10, 26, 2])"]},"metadata":{},"execution_count":85}]},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/educbd/ML_/main/h.jpg)"],"metadata":{"id":"k2i-qZzQZpRF"},"id":"k2i-qZzQZpRF"},{"cell_type":"markdown","source":["## **2) Changes to input  w**\n","```\n"," - w = 20\n","```"],"metadata":{"id":"tDodbkIXbbwH"},"id":"tDodbkIXbbwH"},{"cell_type":"code","source":["input_shape = (4, 28, 20, 3)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=2,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 28, 20, 3))(x)\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-CVehfXbkZn","executionInfo":{"status":"ok","timestamp":1697947362967,"user_tz":240,"elapsed":128,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"4d66635b-40c9-4e37-a320-cadf49dbdee5"},"id":"l-CVehfXbkZn","execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 26, 18, 2])"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/educbd/ML_/main/w.jpg)"],"metadata":{"id":"1VKrglwrdYuh"},"id":"1VKrglwrdYuh"},{"cell_type":"markdown","source":["## **3) Changes to input  nc**\n","```\n"," - nc = 5\n","```"],"metadata":{"id":"yOqREikSdvsA"},"id":"yOqREikSdvsA"},{"cell_type":"code","source":["input_shape = (4, 28, 28, 5)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=2,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 28, 28, 5))(x)\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vTM7zwTPeBFu","executionInfo":{"status":"ok","timestamp":1697947362967,"user_tz":240,"elapsed":126,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"451bd293-6040-4ab9-aa7e-e23f32e1b6de"},"id":"vTM7zwTPeBFu","execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 26, 26, 2])"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/educbd/ML_/main/nc.jpg)"],"metadata":{"id":"bR1VPbuSfkDj"},"id":"bR1VPbuSfkDj"},{"cell_type":"markdown","source":["## **4) Changes to number of filters**\n","```\n"," - filters = 3\n","```"],"metadata":{"id":"Kuizj58-hEO7"},"id":"Kuizj58-hEO7"},{"cell_type":"code","source":["input_shape = (4, 28, 28, 3)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=3,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 28, 28, 3))(x)\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F3Pk0k7phUZ7","executionInfo":{"status":"ok","timestamp":1697947362968,"user_tz":240,"elapsed":3,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"e2c04681-2ec1-4d4e-9e89-0a4bb9fc225c"},"id":"F3Pk0k7phUZ7","execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 26, 26, 3])"]},"metadata":{},"execution_count":66}]},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/educbd/ML_/main/filters.jpg)"],"metadata":{"id":"OJlybkuVicEh"},"id":"OJlybkuVicEh"},{"cell_type":"markdown","source":["## **5) Changes to kernel size**\n","```\n"," - kernel_size = 4\n","```"],"metadata":{"id":"eA7eQlTQjevh"},"id":"eA7eQlTQjevh"},{"cell_type":"code","source":["input_shape = (4, 28, 28, 3)\n","\n","x = tf.random.normal(input_shape)\n","\n","y = Conv2D(filters=2,\n","           kernel_size=(4, 4),\n","           strides=1,\n","           padding='valid',\n","           input_shape=(None, 28, 28, 3))(x)\n","y.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vu8LPGZnjuxQ","executionInfo":{"status":"ok","timestamp":1697947519598,"user_tz":240,"elapsed":140,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"5deb4118-2112-4ba1-a2d1-9419588f07ea"},"id":"vu8LPGZnjuxQ","execution_count":86,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([4, 25, 25, 2])"]},"metadata":{},"execution_count":86}]},{"cell_type":"markdown","source":["![](https://raw.githubusercontent.com/educbd/ML_/main/ker.jpg)"],"metadata":{"id":"DqLdrwmdloR6"},"id":"DqLdrwmdloR6"},{"cell_type":"markdown","source":["## **6) Changes to kernel size**\n","```\n"," - kernel_size = 4\n","```"],"metadata":{"id":"NSq8yZOblz5e"},"id":"NSq8yZOblz5e"},{"cell_type":"markdown","id":"dac958f2","metadata":{"id":"dac958f2"},"source":["## MNIST Revisited\n","\n","Let's now revisit our MNIST. Knowing that the data contains 2-dimensional images of handwritten digits, we should be able to apply what we've learned about convolutions. Thus, in this section, we will create a convolutional neural network (CNN or convnet) for this data set."]},{"cell_type":"code","execution_count":67,"id":"12e2399a","metadata":{"id":"12e2399a","executionInfo":{"status":"ok","timestamp":1697947363306,"user_tz":240,"elapsed":340,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["from tensorflow.keras.datasets import mnist\n","import numpy as np\n","\n","(train_data, train_labels), (test_data, test_labels) = mnist.load_data()"]},{"cell_type":"code","execution_count":68,"id":"a07d1bca","metadata":{"id":"a07d1bca","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363306,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"9a2a16a5-94b6-45df-cfd3-a2055f898470"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28, 28)"]},"metadata":{},"execution_count":68}],"source":["train_data[0].shape"]},{"cell_type":"code","execution_count":69,"id":"fd87f235","metadata":{"id":"fd87f235","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363306,"user_tz":240,"elapsed":3,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"ccb43da9-db9d-438b-e517-a9cf54cd148b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"]},"metadata":{},"execution_count":69}],"source":["train_labels[:10]"]},{"cell_type":"markdown","id":"0933573f","metadata":{"id":"0933573f"},"source":["This time we are going to use a **validation set** to monitor our training progress. We can also use this validation set for *hyperparameter tuning*. Remember, using the validation set allows us to keep the *test set* to gauge how well our final model should do in the real world; that is, the final model only sees the test data once."]},{"cell_type":"code","execution_count":70,"id":"c20c7956","metadata":{"id":"c20c7956","executionInfo":{"status":"ok","timestamp":1697947363307,"user_tz":240,"elapsed":3,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["# Use the first 10,000 samples of our training data as our validation set\n","val_data = train_data[:10000]\n","val_labels = train_labels[:10000]\n","\n","# Use the remainder of the original training data for actual training\n","partial_train_data = train_data[10000:]\n","partial_train_labels = train_labels[10000:]"]},{"cell_type":"code","execution_count":71,"id":"cd053ac2","metadata":{"id":"cd053ac2","executionInfo":{"status":"ok","timestamp":1697947363542,"user_tz":240,"elapsed":238,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["# Scale the pixel values so they lie in the range of 0-1\n","partial_train_data = partial_train_data / 255.\n","val_data = val_data / 255.\n","test_data = test_data /255."]},{"cell_type":"markdown","id":"7205cd6d","metadata":{"id":"7205cd6d"},"source":["Note that our data currently has 3 dimensions: `(samples, height, width)`."]},{"cell_type":"code","execution_count":72,"id":"9d383bcb","metadata":{"id":"9d383bcb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363542,"user_tz":240,"elapsed":6,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"51d6daa2-2c02-4f89-ca9f-acb86b693e32"},"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 28, 28)\n","(10000, 28, 28)\n","(10000, 28, 28)\n"]}],"source":["print(partial_train_data.shape)\n","print(val_data.shape)\n","print(test_data.shape)"]},{"cell_type":"code","execution_count":73,"id":"9d9a56ac","metadata":{"id":"9d9a56ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363542,"user_tz":240,"elapsed":5,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"8a31d27e-dd27-4f7e-f969-71063226d89a"},"outputs":[{"output_type":"stream","name":"stdout","text":["(50000,)\n","(10000,)\n","(10000,)\n"]}],"source":["print(partial_train_labels.shape)\n","print(val_labels.shape)\n","print(test_labels.shape)"]},{"cell_type":"markdown","id":"8587a274","metadata":{"id":"8587a274"},"source":["Our convolutional neural network will expect 4-dimensional data: `(batch_size, height, width, channels)`. Note that depending on how you decide to update the parameters of the network, `batch_size` could equal the number of `samples` (as in *batch gradient descent*), or it could equal a single sample (as in *stochastic gradient descent*, or it can equal the batch size (as in *mini-batch gradient descent*).\n","\n","We can use a NumPy function to add this dimension."]},{"cell_type":"code","execution_count":74,"id":"acbea7c1","metadata":{"id":"acbea7c1","executionInfo":{"status":"ok","timestamp":1697947363542,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["partial_train_data = np.expand_dims(partial_train_data, axis=3)\n","val_data = np.expand_dims(val_data, axis=3)\n","test_data = np.expand_dims(test_data, axis=3)"]},{"cell_type":"code","execution_count":75,"id":"536f781e","metadata":{"id":"536f781e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363542,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"262bfd92-377a-4d85-e603-78242eb96db6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 28, 28, 1)\n","(10000, 28, 28, 1)\n","(10000, 28, 28, 1)\n"]}],"source":["print(partial_train_data.shape)\n","print(val_data.shape)\n","print(test_data.shape)"]},{"cell_type":"markdown","id":"638d8eeb","metadata":{"id":"638d8eeb"},"source":["Note how a fourth dimension was added to our data. This dimension corresponds to the number of channels in our input data. Here it is 1, since the images are all greyscale. It would be 3 if the images were RGB. Also note, that the convention here is *channels last*, as opposed to *channels first*."]},{"cell_type":"markdown","id":"76d9b7e4","metadata":{"id":"76d9b7e4"},"source":["As in Lab 1, we need to convert our label data to the correct format."]},{"cell_type":"code","execution_count":76,"id":"6f1846a1","metadata":{"id":"6f1846a1","executionInfo":{"status":"ok","timestamp":1697947363543,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","\n","partial_train_labels = to_categorical(partial_train_labels)\n","val_labels = to_categorical(val_labels)\n","test_labels = to_categorical(test_labels)"]},{"cell_type":"code","execution_count":77,"id":"c78c4ca3","metadata":{"id":"c78c4ca3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363650,"user_tz":240,"elapsed":110,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"a1d7b8d0-a886-4ed6-d895-54151236b341"},"outputs":[{"output_type":"stream","name":"stdout","text":["(50000, 10)\n","(10000, 10)\n","(10000, 10)\n"]}],"source":["print(partial_train_labels.shape)\n","print(val_labels.shape)\n","print(test_labels.shape)"]},{"cell_type":"markdown","id":"da4a79f2","metadata":{"id":"da4a79f2"},"source":["We will now import the necessary modules for building our convolutional neural network. Since we are using Keras's sequential API we need to import the `Sequential` module. The remaining 3 imports will help us build the layers of our CNN. `Conv2D` creates the convolutional layers we have been discussing in the lectures. `Flatten` is used to create a 1 dimensional vector so we can feed the output of our convolutional layers to the fully-connected layers. We used NumPy's `reshape` function to do this flattening in Lab 1. And the `Dense` layer is the same as what we used in Lab 1."]},{"cell_type":"code","execution_count":78,"id":"9f8ac061","metadata":{"id":"9f8ac061","executionInfo":{"status":"ok","timestamp":1697947363651,"user_tz":240,"elapsed":2,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, Flatten, Dense"]},{"cell_type":"markdown","id":"b604145d","metadata":{"id":"b604145d"},"source":["We are going to use a slightly different approach to building our network than we did in Lab 1. Here we will directly add a *list of layers* to the `Sequential()` object. That is, we put all our layers inside square brackets `[...]` and put this inside the `Sequential( [...] )` object to create our model. In Lab 1 we used the `.add()` method to add individual layers to our `Sequential()` object that we initialized without any layers."]},{"cell_type":"code","execution_count":79,"id":"79abd458","metadata":{"id":"79abd458","executionInfo":{"status":"ok","timestamp":1697947363767,"user_tz":240,"elapsed":118,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["model = Sequential([\n","    Conv2D(filters=32,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='same',\n","           activation='relu',\n","           input_shape=(28, 28, 1)),\n","    Conv2D(filters=32,\n","           kernel_size=(3, 3),\n","           strides=2,\n","           padding='valid',\n","           activation='relu'),\n","    Conv2D(filters=64,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='same',\n","          activation='relu'),\n","    Conv2D(filters=64,\n","           kernel_size=(3, 3),\n","           strides=1,\n","           padding='valid',\n","           activation='relu'),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dense(10, activation='softmax')\n","])"]},{"cell_type":"markdown","id":"79ca17b7","metadata":{"id":"79ca17b7"},"source":["It is often helpful to see the tensor shapes and number of parameters per layer. We can get this information by using the `.summary()` method."]},{"cell_type":"code","execution_count":80,"id":"cfd3649d","metadata":{"id":"cfd3649d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697947363768,"user_tz":240,"elapsed":9,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}},"outputId":"77c81526-c592-4c0a-fa2e-99a1e2833c03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_23 (Conv2D)          (None, 28, 28, 32)        320       \n","                                                                 \n"," conv2d_24 (Conv2D)          (None, 13, 13, 32)        9248      \n","                                                                 \n"," conv2d_25 (Conv2D)          (None, 13, 13, 64)        18496     \n","                                                                 \n"," conv2d_26 (Conv2D)          (None, 11, 11, 64)        36928     \n","                                                                 \n"," flatten_1 (Flatten)         (None, 7744)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 128)               991360    \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                1290      \n","                                                                 \n","=================================================================\n","Total params: 1057642 (4.03 MB)\n","Trainable params: 1057642 (4.03 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"a59f6c82","metadata":{"id":"a59f6c82"},"source":["We are still tackling the same type of problem (multi-class classification) so the same loss and metrics will work for us here. The optimizer `rmsprop` is the same as we used before and can be taken as the default method (or recipe) to try out for updating the model parameters."]},{"cell_type":"code","execution_count":81,"id":"192f7772","metadata":{"id":"192f7772","executionInfo":{"status":"ok","timestamp":1697947363768,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["model.compile(optimizer='rmsprop',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])"]},{"cell_type":"markdown","id":"3bce024b","metadata":{"id":"3bce024b"},"source":["We now fit our model to the remaining training data (the original training data minus the validation data). You will now see that *loss* and *accuracy* get updated for each batch of images (here set to 256) but the *validation loss* and *validation accuracy* get updated after each *epoch*. Note that the *validation data* is not being used to train the model. Each batch of the training data is used to update the parameters and then, once we have gone through all of the samples in our training data (that is, all the samples in `partial_train_data`) the model is used to make predictions for the validation set. From those predictions the validation loss and accuracy are calculated.\n","\n","Each epoch of training should take 30-50s to complete."]},{"cell_type":"code","execution_count":59,"id":"8f314531","metadata":{"id":"8f314531","colab":{"base_uri":"https://localhost:8080/","height":517},"outputId":"7c8c8933-94dd-488d-f539-f9c5b03a76b4","executionInfo":{"status":"error","timestamp":1697947362121,"user_tz":240,"elapsed":75770,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","196/196 [==============================] - 107s 540ms/step - loss: 0.3162 - accuracy: 0.9078 - val_loss: 0.1087 - val_accuracy: 0.9669\n","Epoch 2/10\n"," 29/196 [===>..........................] - ETA: 1:21 - loss: 0.0607 - accuracy: 0.9815"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-9e2b6d43c86e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(partial_train_data,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mpartial_train_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["history = model.fit(partial_train_data,\n","                    partial_train_labels,\n","                    epochs=10,\n","                    batch_size=256,\n","                    validation_data=(val_data, val_labels),\n","                    verbose=1)"]},{"cell_type":"markdown","id":"37a38a57","metadata":{"id":"37a38a57"},"source":["The values for the training loss and accuracy, as well as the validation loss and accuracy, are stored in the `history` variable. You can see the structure of the dictionary that stores this information as follows:"]},{"cell_type":"code","execution_count":null,"id":"50091fd0","metadata":{"id":"50091fd0","executionInfo":{"status":"aborted","timestamp":1697947362123,"user_tz":240,"elapsed":3,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["history.history['loss']"]},{"cell_type":"markdown","id":"5d77157e","metadata":{"id":"5d77157e"},"source":["We will now use this information to visualize the progress our network makes on the loss and accuracy as the number of epochs increases."]},{"cell_type":"code","execution_count":null,"id":"4997ef2b","metadata":{"id":"4997ef2b","executionInfo":{"status":"aborted","timestamp":1697947362123,"user_tz":240,"elapsed":3,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["import matplotlib.pyplot as plt  # needed to create our plot\n","\n","history_dict = history.history # the dictionary that has the information on loss and accuracy per epoch\n","\n","loss_values = history_dict['loss']   # training loss\n","val_loss_values = history_dict['val_loss'] # validation loss\n","\n","epochs = range(1, len(loss_values)+1)  #creates list of integers to match the number of epochs of training\n","\n","# code to plot the results\n","plt.plot(epochs, loss_values, 'b', label=\"Training Loss\")\n","plt.plot(epochs, val_loss_values, 'r', label=\"Validation Loss\")\n","plt.title(\"Training and Validation Loss\")\n","plt.xlabel(\"Epochs\")\n","plt.xticks(epochs)\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"f5b78182","metadata":{"id":"f5b78182","executionInfo":{"status":"aborted","timestamp":1697947362124,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":["# As above, but this time we want to visualize the training and validation accuracy\n","acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","\n","plt.plot(epochs, acc_values, 'b', label=\"Training Accuracy\")\n","plt.plot(epochs, val_acc_values, 'r', label=\"Validation Accuracy\")\n","plt.title(\"Training and Validation Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.xticks(epochs)\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","id":"16f89c1b-c3e4-47cc-b083-3dbe056c4926","metadata":{"id":"16f89c1b-c3e4-47cc-b083-3dbe056c4926"},"source":["## Exercise: Change the layers\n","\n","Play around with the **number of filters** and the **filter size** in our model. Note the change in:\n","- number of parameters in the model\n","- training and validation losses and accuracies"]},{"cell_type":"code","source":[],"metadata":{"id":"SyejL1KcgDxd","executionInfo":{"status":"aborted","timestamp":1697947362125,"user_tz":240,"elapsed":4,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"SyejL1KcgDxd","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"duOyt_uigD34","executionInfo":{"status":"aborted","timestamp":1697947362126,"user_tz":240,"elapsed":5,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"duOyt_uigD34","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Toylx6vVgD9_","executionInfo":{"status":"aborted","timestamp":1697947362127,"user_tz":240,"elapsed":6,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"Toylx6vVgD9_","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"9b79813e-127a-48fc-aa92-c4b4453a6da4","metadata":{"tags":[],"id":"9b79813e-127a-48fc-aa92-c4b4453a6da4"},"source":["### Exercise: Early Stopping\n","\n","When you have a final model, train it until the validation loss stops decreasing. At this point, the model will have stopped learning and will start to memorize the training data. The model may be starting to overfit. Note the number of epochs at which this happens.  One way to avoid this overfitting is called *early stopping*.  \n","\n","Try implementing early stopping for our model:\n","- use the validation loss plot to determine which epoch corresponds to when the model stops learning\n","    - if it so happens that the validation loss continues going down for all 10 epochs, then increase the number of epochs in the original code to 20\n","- use the complete training set (no validation set)\n","- scale this training set\n","- expand its dimensions to 4\n","- use the same model, and same optimizer, loss and metrics\n","- fit the model to the complete training set (no validation set)\n","- evaluate the trained model on the test data\n"]},{"cell_type":"code","execution_count":null,"id":"c9226968","metadata":{"id":"c9226968","executionInfo":{"status":"aborted","timestamp":1697947362127,"user_tz":240,"elapsed":6,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"QnP6llR3gFBi","executionInfo":{"status":"aborted","timestamp":1697947362128,"user_tz":240,"elapsed":7,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"QnP6llR3gFBi","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uwmVS7CJgFIn","executionInfo":{"status":"aborted","timestamp":1697947362128,"user_tz":240,"elapsed":7,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"uwmVS7CJgFIn","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"191cecc7-466c-4fee-80f5-31c8f80c9f72","metadata":{"id":"191cecc7-466c-4fee-80f5-31c8f80c9f72"},"source":["### Exercise: Early Stopping with Callbacks\n","\n","Now try to implement early stopping using the Keras [callback](https://keras.io/api/callbacks/early_stopping/) functionality. In this case, you will need to use the validation data, because you want the early stopping to occur as a result of Keras monitoring the validation loss."]},{"cell_type":"code","execution_count":null,"id":"b7ae1ec6-d28f-434f-a503-04d4d18bd892","metadata":{"id":"b7ae1ec6-d28f-434f-a503-04d4d18bd892","executionInfo":{"status":"aborted","timestamp":1697947362128,"user_tz":240,"elapsed":7,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"outputs":[],"source":[]},{"cell_type":"code","source":[],"metadata":{"id":"b2l9VpL8gGqd","executionInfo":{"status":"aborted","timestamp":1697947362229,"user_tz":240,"elapsed":0,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"b2l9VpL8gGqd","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"93nWI5YhgGvv","executionInfo":{"status":"aborted","timestamp":1697947362230,"user_tz":240,"elapsed":1,"user":{"displayName":"Eduardo Chavez","userId":"17400250430576262443"}}},"id":"93nWI5YhgGvv","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}